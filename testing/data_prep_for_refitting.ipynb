{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e396bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 13:09:18,726 - WARNING - configdefaults - g++ not available, if using conda: `conda install gxx`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 13:09:18,726 - WARNING - configdefaults - g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "2025-06-09 13:09:33,377 - INFO - preview - arviz_base not installed\n",
      "2025-06-09 13:09:33,387 - INFO - preview - arviz_stats not installed\n",
      "2025-06-09 13:09:33,393 - INFO - preview - arviz_plots not installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Add the baseball_simulator directory to path\n",
    "sys.path.append('./baseball_simulator')\n",
    "\n",
    "# Import configuration and data processing modules\n",
    "import config\n",
    "from data_fetcher import fetch_statcast_data, fetch_defensive_stats, fetch_park_factors\n",
    "from data_processor import (\n",
    "    process_statcast_data,\n",
    "    create_helper_columns,\n",
    "    calculate_batter_daily_totals,\n",
    "    calculate_pitcher_daily_totals,\n",
    "    calculate_ballasted_batter_stats,\n",
    "    calculate_ballasted_pitcher_stats,\n",
    "    join_together_final_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a6ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Fetching statcast data...\n",
      "start_dt 2025-06-08\n",
      "end_dt 2025-06-09\n",
      "Warning: no date range supplied, assuming yesterday's date.\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  1.92it/s]c:\\Users\\samba\\OneDrive\\Python_Projects\\first_3_innings_model\\.venv\\Lib\\site-packages\\pybaseball\\datahelpers\\postprocessing.py:59: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  data_copy[column] = data_copy[column].apply(pd.to_datetime, errors='ignore', format=date_format)\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching defensive stats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\samba\\OneDrive\\Python_Projects\\first_3_innings_model\\.venv\\Lib\\site-packages\\pybaseball\\statcast.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat(dataframe_list, axis=0).convert_dtypes(convert_string=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fetch_defensive_stats() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutcome distribution:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinal_df[\u001b[33m'\u001b[39m\u001b[33moutcome_cat\u001b[39m\u001b[33m'\u001b[39m].value_counts().sort_index()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m final_df = \u001b[43mprepare_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m final_df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mprepare_training_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m statcast_df = fetch_statcast_data()\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFetching defensive stats...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m defensive_df = \u001b[43mfetch_defensive_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2021\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2024\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fetching defensive stats for 2021-2024\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFetching park factors...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m park_factors_df = fetch_park_factors()\n",
      "\u001b[31mTypeError\u001b[39m: fetch_defensive_stats() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "def prepare_training_data():\n",
    "    \"\"\"\n",
    "    Prepare training data using the exact same pipeline as the simulator.\n",
    "    Only uses 2023-2024 data for model fitting.\n",
    "    \"\"\"\n",
    "    print(\"Starting data preparation...\")\n",
    "    \n",
    "    # Fetch raw data (2021-2024 for ballasting, but will filter to 2023-2024 for training)\n",
    "    print(\"Fetching statcast data...\")\n",
    "    statcast_df = fetch_statcast_data()\n",
    "    \n",
    "    print(\"Fetching defensive stats...\")\n",
    "    defensive_df = fetch_defensive_stats(2021, 2024)  # Fetching defensive stats for 2021-2024\n",
    "    \n",
    "    print(\"Fetching park factors...\")\n",
    "    park_factors_df = fetch_park_factors()\n",
    "    \n",
    "    # Process statcast data\n",
    "    print(\"Processing statcast data...\")\n",
    "    processed_df = process_statcast_data(statcast_df)\n",
    "    \n",
    "    # Create helper columns\n",
    "    print(\"Creating helper columns...\")\n",
    "    processed_df = create_helper_columns(processed_df)\n",
    "    \n",
    "    # Calculate daily totals\n",
    "    print(\"Calculating batter daily totals...\")\n",
    "    batter_daily_df = calculate_batter_daily_totals(processed_df)\n",
    "    \n",
    "    print(\"Calculating pitcher daily totals...\")\n",
    "    pitcher_daily_df = calculate_pitcher_daily_totals(processed_df)\n",
    "    \n",
    "    # Calculate ballasted stats\n",
    "    print(\"Calculating ballasted batter stats...\")\n",
    "    ballasted_batter_df = calculate_ballasted_batter_stats(batter_daily_df)\n",
    "    \n",
    "    print(\"Calculating ballasted pitcher stats...\")\n",
    "    ballasted_pitcher_df = calculate_ballasted_pitcher_stats(pitcher_daily_df)\n",
    "    \n",
    "    # Join everything together\n",
    "    print(\"Joining final dataset...\")\n",
    "    final_df = join_together_final_df(\n",
    "        processed_df, \n",
    "        ballasted_batter_df, \n",
    "        ballasted_pitcher_df, \n",
    "        defensive_df, \n",
    "        park_factors_df\n",
    "    )\n",
    "    \n",
    "    # Filter to training requirements\n",
    "    print(\"Applying training filters...\")\n",
    "    \n",
    "    # Filter to innings 1-3 only\n",
    "    final_df = final_df[final_df['inning'] <= 3].copy()\n",
    "    \n",
    "    # Filter to 2023-2024 for training (as specified by user)\n",
    "    training_years = [2023, 2024]\n",
    "    final_df = final_df[final_df['game_year'].isin(training_years)].copy()\n",
    "    \n",
    "    # Remove rows with missing values in predictor columns\n",
    "    final_df = final_df.dropna(subset=config.PREDICTOR_COLS + ['outcome_cat']).copy()\n",
    "    \n",
    "    print(f\"Final training dataset shape: {final_df.shape}\")\n",
    "    print(f\"Training years: {sorted(final_df['game_year'].unique())}\")\n",
    "    print(f\"Outcome distribution:\\n{final_df['outcome_cat'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "final_df = prepare_training_data()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_model_inputs(final_df):\n",
    "    \"\"\"\n",
    "    Prepare model inputs ensuring consistency with simulator expectations.\n",
    "    \"\"\"\n",
    "    print(\"Preparing model inputs...\")\n",
    "    \n",
    "    # Extract features in the exact order expected by simulator\n",
    "    X = final_df[config.PREDICTOR_COLS].to_numpy()\n",
    "    y = final_df['outcome_cat'].to_numpy()\n",
    "    \n",
    "    # Separate continuous and categorical features\n",
    "    n_continuous = len(config.CONTINUOUS_COLS)\n",
    "    X_continuous = X[:, :n_continuous]\n",
    "    X_categorical = X[:, n_continuous:]\n",
    "    \n",
    "    # Scale only continuous features\n",
    "    print(\"Scaling continuous features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_continuous_scaled = scaler.fit_transform(X_continuous)\n",
    "    \n",
    "    # Combine scaled continuous with unscaled categorical\n",
    "    X_final = np.concatenate([X_continuous_scaled, X_categorical], axis=1)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_final.shape}\")\n",
    "    print(f\"Continuous features: {X_continuous.shape[1]}\")\n",
    "    print(f\"Categorical features: {X_categorical.shape[1]}\")\n",
    "    print(f\"Target categories: {len(np.unique(y))}\")\n",
    "    \n",
    "    return X_final, y, scaler\n",
    "\n",
    "def build_and_train_model(X, y):\n",
    "    \"\"\"\n",
    "    Build and train the PyMC multinomial logistic regression model.\n",
    "    \"\"\"\n",
    "    print(\"Building PyMC model...\")\n",
    "    \n",
    "    n_obs, n_predictors = X.shape\n",
    "    n_categories = config.N_CATEGORIES\n",
    "    \n",
    "    print(f\"Model dimensions:\")\n",
    "    print(f\"  Observations: {n_obs}\")\n",
    "    print(f\"  Predictors: {n_predictors}\")\n",
    "    print(f\"  Categories: {n_categories}\")\n",
    "    \n",
    "    with pm.Model() as multi_outcome_model:\n",
    "        # Intercepts (n_categories - 1 free parameters, last category = 0 for identifiability)\n",
    "        intercepts_offset = pm.Normal(\n",
    "            \"intercepts_offset\", \n",
    "            mu=0, \n",
    "            sigma=1.5, \n",
    "            shape=(n_categories - 1,)\n",
    "        )\n",
    "        intercepts = pm.Deterministic(\n",
    "            \"intercepts\", \n",
    "            pt.concatenate([intercepts_offset, pt.zeros(1)])\n",
    "        )\n",
    "        \n",
    "        # Coefficients (n_predictors × (n_categories - 1) free parameters)\n",
    "        betas_offset = pm.Normal(\n",
    "            \"betas_offset\", \n",
    "            mu=0, \n",
    "            sigma=1.0, \n",
    "            shape=(n_predictors, n_categories - 1)\n",
    "        )\n",
    "        betas = pm.Deterministic(\n",
    "            \"betas\", \n",
    "            pt.concatenate([betas_offset, pt.zeros((n_predictors, 1))], axis=1)\n",
    "        )\n",
    "        \n",
    "        # Linear predictor\n",
    "        mu = intercepts + X @ betas\n",
    "        \n",
    "        # Softmax to get probabilities\n",
    "        p = pm.math.softmax(mu, axis=1)\n",
    "        \n",
    "        # Likelihood\n",
    "        outcome = pm.Categorical(\"outcome\", p=p, observed=y)\n",
    "    \n",
    "    # Sample from the model\n",
    "    print(\"Starting MCMC sampling...\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    with multi_outcome_model:\n",
    "        # Use JAX backend for faster sampling if available\n",
    "        try:\n",
    "            idata = pm.sample(\n",
    "                draws=1000,\n",
    "                tune=1000,\n",
    "                chains=2,\n",
    "                target_accept=0.90,\n",
    "                nuts_sampler=\"numpyro\",\n",
    "                random_seed=42\n",
    "            )\n",
    "            print(\"Used JAX/numpyro backend for sampling\")\n",
    "        except Exception as e:\n",
    "            print(f\"JAX backend failed ({e}), falling back to default backend\")\n",
    "            idata = pm.sample(\n",
    "                draws=1000,\n",
    "                tune=1000,\n",
    "                chains=2,\n",
    "                target_accept=0.90,\n",
    "                random_seed=42\n",
    "            )\n",
    "    \n",
    "    return multi_outcome_model, idata\n",
    "\n",
    "def validate_model_convergence(idata):\n",
    "    \"\"\"\n",
    "    Validate model convergence using ArviZ diagnostics.\n",
    "    \"\"\"\n",
    "    print(\"Validating model convergence...\")\n",
    "    \n",
    "    # Check R-hat values\n",
    "    r_hat = az.rhat(idata)\n",
    "    max_r_hat = float(r_hat.max().values)\n",
    "    print(f\"Maximum R-hat: {max_r_hat:.4f}\")\n",
    "    \n",
    "    if max_r_hat > 1.1:\n",
    "        print(\"WARNING: Some R-hat values > 1.1, model may not have converged\")\n",
    "    else:\n",
    "        print(\"✓ All R-hat values < 1.1, good convergence\")\n",
    "    \n",
    "    # Check effective sample size\n",
    "    ess = az.ess(idata)\n",
    "    min_ess = float(ess.min().values)\n",
    "    print(f\"Minimum effective sample size: {min_ess:.0f}\")\n",
    "    \n",
    "    if min_ess < 400:\n",
    "        print(\"WARNING: Some ESS values < 400, consider longer sampling\")\n",
    "    else:\n",
    "        print(\"✓ All ESS values > 400, sufficient sampling\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nModel summary:\")\n",
    "    print(az.summary(idata, round_to=4))\n",
    "    \n",
    "    return max_r_hat < 1.1 and min_ess >= 400\n",
    "\n",
    "def save_model_artifacts(idata, scaler):\n",
    "    \"\"\"\n",
    "    Save model and scaler with the exact filenames expected by ModelLoader.\n",
    "    \"\"\"\n",
    "    print(\"Saving model artifacts...\")\n",
    "    \n",
    "    # Save PyMC inference data\n",
    "    model_filename = \"multi_outcome_model.nc\"\n",
    "    idata.to_netcdf(model_filename)\n",
    "    print(f\"✓ Saved model to: {model_filename}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_filename = \"pa_outcome_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"✓ Saved scaler to: {scaler_filename}\")\n",
    "    \n",
    "    return model_filename, scaler_filename\n",
    "\n",
    "def test_model_loading():\n",
    "    \"\"\"\n",
    "    Test that the saved model can be loaded with the existing ModelLoader.\n",
    "    \"\"\"\n",
    "    print(\"Testing model loading...\")\n",
    "    \n",
    "    try:\n",
    "        from model_loader import ModelLoader\n",
    "        \n",
    "        # Test loading\n",
    "        loader = ModelLoader(base_dir=\"./\")\n",
    "        model, scaler = loader.load_all()\n",
    "        \n",
    "        print(\"✓ Model and scaler loaded successfully with ModelLoader\")\n",
    "        \n",
    "        # Test scaler dimensions\n",
    "        dummy_continuous = np.random.randn(5, len(config.CONTINUOUS_COLS))\n",
    "        scaled = scaler.transform(dummy_continuous)\n",
    "        \n",
    "        if scaled.shape == dummy_continuous.shape:\n",
    "            print(\"✓ Scaler transforms data correctly\")\n",
    "        else:\n",
    "            print(\"✗ Scaler dimension mismatch\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error testing model loading: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to refit the PyMC model.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"REFITTING PYMC MODEL FOR BASEBALL SIMULATOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate configuration\n",
    "    print(f\"Using predictor columns: {len(config.PREDICTOR_COLS)} features\")\n",
    "    print(f\"Continuous features: {len(config.CONTINUOUS_COLS)}\")\n",
    "    print(f\"Categorical features: {len(config.CATEGORICAL_COLS)}\")\n",
    "    print(f\"Number of outcome categories: {config.N_CATEGORIES}\")\n",
    "    print(f\"Training on years: 2023-2024\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare training data\n",
    "        final_df = prepare_training_data()\n",
    "        \n",
    "        # Step 2: Prepare model inputs\n",
    "        X, y, scaler = prepare_model_inputs(final_df)\n",
    "        \n",
    "        # Step 3: Build and train model\n",
    "        model, idata = build_and_train_model(X, y)\n",
    "        \n",
    "        # Step 4: Validate convergence\n",
    "        converged = validate_model_convergence(idata)\n",
    "        \n",
    "        # Step 5: Save artifacts\n",
    "        model_file, scaler_file = save_model_artifacts(idata, scaler)\n",
    "        \n",
    "        # Step 6: Test loading\n",
    "        test_model_loading()\n",
    "        \n",
    "        print()\n",
    "        print(\"=\"*60)\n",
    "        if converged:\n",
    "            print(\"✓ MODEL REFITTING COMPLETED SUCCESSFULLY\")\n",
    "        else:\n",
    "            print(\"⚠ MODEL REFITTING COMPLETED WITH CONVERGENCE WARNINGS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model saved to: {model_file}\")\n",
    "        print(f\"Scaler saved to: {scaler_file}\")\n",
    "        print()\n",
    "        print(\"The model is now ready for use with the baseball simulator.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during model refitting: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
